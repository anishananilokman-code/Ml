import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
import squarify
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (
Â Â Â Â accuracy_score, precision_score, recall_score, f1_score,
Â Â Â Â mean_absolute_error, mean_squared_error, classification_report
)
import xgboost as xgb
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Page config
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Malaysia Labour Market Dashboard", layout="wide")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Load & clean data
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_data():
Â Â Â Â try:
Â Â Â Â Â Â Â Â df = pd.read_csv('clean_data.csv')
Â Â Â Â Â Â Â Â df['date'] = pd.to_datetime(df['date'], errors='coerce')
Â Â Â Â Â Â Â Â df = df.dropna(subset=['date'])
Â Â Â Â Â Â Â Â df = df[df['employment'] > 500].copy() # remove percentage-like rows
Â Â Â Â Â Â Â Â df = df.drop_duplicates()
Â Â Â Â Â Â Â Â df = df.sort_values('date').groupby(['date', 'sector']).first().reset_index()
Â Â Â Â Â Â Â Â return df
Â Â Â Â except FileNotFoundError:
Â Â Â Â Â Â Â Â st.error("clean_data.csv not found.")
Â Â Â Â Â Â Â Â st.stop()
data = load_data()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Sidebar filters + debug years
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
Â Â Â Â st.header("Filters")
Â Â Â 
Â Â Â Â st.caption("Available years in dataset:")
Â Â Â Â st.write(data['date'].dt.year.value_counts().sort_index())
Â Â Â 
Â Â Â Â min_year = int(data['date'].dt.year.min())
Â Â Â Â max_year = int(data['date'].dt.year.max())
Â Â Â Â year_range = st.slider("Year Range", min_year, max_year, (2016, 2022))
Â Â Â Â all_sectors = sorted(data['sector'].unique())
Â Â Â Â selected_sectors = st.multiselect("Sectors", all_sectors, default=all_sectors)
filtered_data = data[
Â Â Â Â (data['date'].dt.year.between(year_range[0], year_range[1])) &
Â Â Â Â (data['sector'].isin(selected_sectors))
].copy()
if filtered_data.empty:
Â Â Â Â st.warning(f"No data for {year_range[0]}â€“{year_range[1]}. Check available years above.")
Â Â Â Â st.stop()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Train & evaluate models
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def train_and_evaluate():
Â Â Â Â df = data.copy()
Â Â Â Â le = LabelEncoder()
Â Â Â Â y = le.fit_transform(df['sector'])
Â Â Â Â features = [
Â Â Â Â Â Â Â Â 'gdp', 'employment', 'hours', 'output_hour', 'output_employment',
Â Â Â Â Â Â Â Â 'employed_employer', 'employed_employee', 'employed_own_account', 'employed_unpaid_family'
Â Â Â Â ]
Â Â Â Â features = [f for f in features if f in df.columns]
Â Â Â Â X = df[features].fillna(0)
Â Â Â Â X_train, X_test, y_train, y_test = train_test_split(
Â Â Â Â Â Â Â Â X, y, test_size=0.2, random_state=42, stratify=y
Â Â Â Â )
Â Â Â Â scaler = StandardScaler()
Â Â Â Â X_train_s = scaler.fit_transform(X_train)
Â Â Â Â X_test_s = scaler.transform(X_test)
Â Â Â Â models = {
Â Â Â Â Â Â Â Â "XGBoost": xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),
Â Â Â Â Â Â Â Â "Random Forest": RandomForestClassifier(random_state=42),
Â Â Â Â Â Â Â Â "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
Â Â Â Â Â Â Â Â "SVM": SVC(probability=True, random_state=42)
Â Â Â Â }
Â Â Â Â metrics_list = []
Â Â Â Â preds_dict = {}
Â Â Â Â for name, model in models.items():
Â Â Â Â Â Â Â Â model.fit(X_train_s, y_train)
Â Â Â Â Â Â Â Â y_pred = model.predict(X_test_s)
Â Â Â Â Â Â Â Â preds_dict[name] = y_pred
Â Â Â Â Â Â Â Â acc = accuracy_score(y_test, y_pred)
Â Â Â Â Â Â Â Â prec = precision_score(y_test, y_pred, average='macro', zero_division=0)
Â Â Â Â Â Â Â Â rec = recall_score(y_test, y_pred, average='macro', zero_division=0)
Â Â Â Â Â Â Â Â f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
Â Â Â Â Â Â Â Â mae = mean_absolute_error(y_test, y_pred)
Â Â Â Â Â Â Â Â rmse = np.sqrt(mean_squared_error(y_test, y_pred))
Â Â Â Â Â Â Â Â metrics_list.append({
Â Â Â Â Â Â Â Â Â Â Â Â 'Model': name,
Â Â Â Â Â Â Â Â Â Â Â Â 'Accuracy': acc,
Â Â Â Â Â Â Â Â Â Â Â Â 'Precision (macro)': prec,
Â Â Â Â Â Â Â Â Â Â Â Â 'Recall (macro)': rec,
Â Â Â Â Â Â Â Â Â Â Â Â 'F1-Score (macro)': f1,
Â Â Â Â Â Â Â Â Â Â Â Â 'MAE': mae,
Â Â Â Â Â Â Â Â Â Â Â Â 'RMSE': rmse
Â Â Â Â Â Â Â Â })
Â Â Â Â metrics_df = pd.DataFrame(metrics_list)
Â Â Â Â best_model_name = metrics_df.loc[metrics_df['F1-Score (macro)'].idxmax(), 'Model']
Â Â Â Â best_model = models[best_model_name]
Â Â Â Â return {
Â Â Â Â Â Â Â Â 'best_model': best_model,
Â Â Â Â Â Â Â Â 'best_name': best_model_name,
Â Â Â Â Â Â Â Â 'scaler': scaler,
Â Â Â Â Â Â Â Â 'encoder': le,
Â Â Â Â Â Â Â Â 'metrics_df': metrics_df,
Â Â Â Â Â Â Â Â 'y_test': y_test,
Â Â Â Â Â Â Â Â 'predictions': preds_dict
Â Â Â Â }
model_results = train_and_evaluate()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main title
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("Malaysia Labour Market Dashboard â€“ MSIC Sectors")
st.caption("GDP, Employment & Sector Classification Analysis")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Tabs
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab_kpi, tab_trends, tab_model, tab_predict, tab_data = st.tabs([
Â Â Â Â "ðŸ“Š Key Indicators",
Â Â Â Â "ðŸ“ˆ Trends & EDA",
Â Â Â Â "ðŸ“Š Model Performance",
Â Â Â Â "ðŸ”® Sector Prediction",
Â Â Â Â "ðŸ“‹ Data Table"
])
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Tab 1: Key Indicators
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_kpi:
Â Â Â Â st.subheader(f"Key Indicators â€“ Latest year in filter ({filtered_data['date'].dt.year.max()})")
Â Â Â Â latest_year = filtered_data['date'].dt.year.max()
Â Â Â Â latest = filtered_data[filtered_data['date'].dt.year == latest_year]
Â Â Â Â if not latest.empty:
Â Â Â Â Â Â Â Â tot_emp = latest['employment'].sum()
Â Â Â Â Â Â Â Â tot_gdp = latest['gdp'].sum()
Â Â Â Â Â Â Â Â struc = latest[[
Â Â Â Â Â Â Â Â Â Â Â Â 'employed_employee', 'employed_employer',
Â Â Â Â Â Â Â Â Â Â Â Â 'employed_own_account', 'employed_unpaid_family'
Â Â Â Â Â Â Â Â ]].sum()
Â Â Â Â Â Â Â Â tot_workers = struc.sum() or 1
Â Â Â Â Â Â Â Â c1, c2, c3, c4, c5 = st.columns(5)
Â Â Â Â Â Â Â Â c1.metric("Total Employment", f"{tot_emp:,.0f}")
Â Â Â Â Â Â Â Â c2.metric("Total GDP", f"RM {tot_gdp:,.0f} mil")
Â Â Â Â Â Â Â Â c3.metric("% Employees", f"{struc['employed_employee']/tot_workers*100:.1f}%")
Â Â Â Â Â Â Â Â c4.metric("% Own-account", f"{struc['employed_own_account']/tot_workers*100:.1f}%")
Â Â Â Â Â Â Â Â c5.metric("% Employers", f"{struc['employed_employer']/tot_workers*100:.1f}%")
Â Â Â Â emp_share = filtered_data.groupby('sector')['employment'].sum().reset_index()
Â Â Â Â fig_pie = px.pie(emp_share, values='employment', names='sector',
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â title="Share of Total Employment by Sector", hole=0.4)
Â Â Â Â st.plotly_chart(fig_pie, use_container_width=True)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Tab 2: Trends & EDA (with your requested charts)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_trends:
Â Â Â Â st.subheader("Sector Productivity & Correlations")
Â Â Â Â # 1. Treemap + Pie + Bar combo: Share of Total Employment by Sector
Â Â Â Â sector_emp_total = filtered_data.groupby('sector')['employment'].sum().sort_values(ascending=False)
Â Â Â Â percentages = (sector_emp_total / sector_emp_total.sum() * 100).round(1)
Â Â Â Â col_left, col_right = st.columns([2, 1])
Â Â Â Â with col_left:
Â Â Â Â Â Â Â Â # Treemap
Â Â Â Â Â Â Â Â fig_treemap, ax = plt.subplots(figsize=(10, 8))
Â Â Â Â Â Â Â Â squarify.plot(sizes=sector_emp_total, label=sector_emp_total.index,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â value=percentages.apply(lambda x: f'{x}%'), alpha=0.8,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â color=plt.cm.Blues(np.linspace(0.4, 0.9, len(sector_emp_total))))
Â Â Â Â Â Â Â Â plt.title('Share of Total Employment by Sector (Treemap)', fontsize=14)
Â Â Â Â Â Â Â Â plt.axis('off')
Â Â Â Â Â Â Â Â st.pyplot(fig_treemap)
Â Â Â Â with col_right:
Â Â Â Â Â Â Â Â # Pie + Horizontal Bar side by side (smaller)
Â Â Â Â Â Â Â Â fig_combo, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))
Â Â Â Â Â Â Â Â # Pie
Â Â Â Â Â Â Â Â ax1.pie(sector_emp_total, labels=sector_emp_total.index, autopct='%1.1f%%',
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â startangle=90, colors=plt.cm.Set3(np.linspace(0, 1, len(sector_emp_total))))
Â Â Â Â Â Â Â Â ax1.set_title('Employment Share (%)')
Â Â Â Â Â Â Â Â # Horizontal Bar
Â Â Â Â Â Â Â Â ax2.barh(sector_emp_total.index, sector_emp_total.values,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â color=plt.cm.Set3(np.linspace(0, 1, len(sector_emp_total))))
Â Â Â Â Â Â Â Â ax2.set_xlabel('Number of Employees')
Â Â Â Â Â Â Â Â ax2.set_title('Absolute Employment')
Â Â Â Â Â Â Â Â ax2.invert_yaxis()
Â Â Â Â Â Â Â Â plt.tight_layout()
Â Â Â Â Â Â Â Â st.pyplot(fig_combo)
Â Â Â Â # 2. Average Output per Hour by Sector
Â Â Â Â sector_hour_prod = filtered_data.groupby('sector')['output_hour'].mean().sort_values(ascending=False)
Â Â Â Â fig_hour = px.bar(
Â Â Â Â Â Â Â Â sector_hour_prod.reset_index(),
Â Â Â Â Â Â Â Â x='sector',
Â Â Â Â Â Â Â Â y='output_hour',
Â Â Â Â Â Â Â Â title='Average Output per Hour by Sector',
Â Â Â Â Â Â Â Â labels={'output_hour': 'Output per Hour', 'sector': 'Sector'}
Â Â Â Â )
Â Â Â Â st.plotly_chart(fig_hour, use_container_width=True)
Â Â Â Â # 3. 4 Scatter plots with linear fit
Â Â Â Â st.subheader("GDP vs Key Variables")
Â Â Â Â fig_scatter, axes = plt.subplots(1, 4, figsize=(20, 5))
Â Â Â Â # GDP vs Employment
Â Â Â Â axes[0].scatter(filtered_data['gdp'], filtered_data['employment'], color='steelblue', alpha=0.6)
Â Â Â Â z = np.polyfit(filtered_data['gdp'], filtered_data['employment'], 1)
Â Â Â Â p = np.poly1d(z)
Â Â Â Â axes[0].plot(filtered_data['gdp'], p(filtered_data['gdp']), color='darkred', linestyle='--')
Â Â Â Â axes[0].set_xlabel('GDP')
Â Â Â Â axes[0].set_ylabel('Employment')
Â Â Â Â axes[0].set_title('GDP vs Employment')
Â Â Â Â axes[0].grid(True, alpha=0.3)
Â Â Â Â # GDP vs Total Working Hours
Â Â Â Â axes[1].scatter(filtered_data['gdp'], filtered_data['hours'], color='green', alpha=0.6)
Â Â Â Â z = np.polyfit(filtered_data['gdp'], filtered_data['hours'], 1)
Â Â Â Â p = np.poly1d(z)
Â Â Â Â axes[1].plot(filtered_data['gdp'], p(filtered_data['gdp']), color='darkred', linestyle='--')
Â Â Â Â axes[1].set_xlabel('GDP')
Â Â Â Â axes[1].set_ylabel('Total Working Hours')
Â Â Â Â axes[1].set_title('GDP vs Total Working Hours')
Â Â Â Â axes[1].grid(True, alpha=0.3)
Â Â Â Â # GDP vs Output per Hour
Â Â Â Â axes[2].scatter(filtered_data['gdp'], filtered_data['output_hour'], color='orange', alpha=0.6)
Â Â Â Â axes[2].set_xlabel('GDP')
Â Â Â Â axes[2].set_ylabel('Output per Hour')
Â Â Â Â axes[2].set_title('GDP vs Output per Hour')
Â Â Â Â axes[2].grid(True, alpha=0.3)
Â Â Â Â # GDP vs Output per Employee
Â Â Â Â axes[3].scatter(filtered_data['gdp'], filtered_data['output_employment'], color='purple', alpha=0.6)
Â Â Â Â axes[3].set_xlabel('GDP')
Â Â Â Â axes[3].set_ylabel('Output per Employee')
Â Â Â Â axes[3].set_title('GDP vs Output per Employee')
Â Â Â Â axes[3].grid(True, alpha=0.3)
Â Â Â Â plt.tight_layout()
Â Â Â Â st.pyplot(fig_scatter)
Â Â Â Â # 4. Correlation Heatmap
Â Â Â Â st.subheader("Correlation Heatmap")
Â Â Â Â key_vars = ['gdp', 'employment', 'hours', 'output_hour', 'output_employment']
Â Â Â Â key_vars = [v for v in key_vars if v in filtered_data.columns]
Â Â Â Â if len(key_vars) >= 2:
Â Â Â Â Â Â Â Â corr = filtered_data[key_vars].corr()
Â Â Â Â Â Â Â Â fig_heat, ax = plt.subplots(figsize=(8, 6))
Â Â Â Â Â Â Â Â sns.heatmap(corr, annot=True, fmt=".2f", cmap="plasma", ax=ax)
Â Â Â Â Â Â Â Â plt.title("Correlation Heatmap")
Â Â Â Â Â Â Â Â st.pyplot(fig_heat)
Â Â Â Â Â Â Â Â st.markdown("**Correlation Matrix (numeric values):**")
Â Â Â Â Â Â Â Â st.dataframe(corr.style.format("{:.3f}"), use_container_width=True)
Â Â Â Â else:
Â Â Â Â Â Â Â Â st.info("Not enough variables for correlation heatmap.")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Tab 3: Model Performance
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_model:
Â Â Â Â st.subheader("Model Performance â€“ Detailed Evaluation")
Â Â Â Â df_metrics = model_results['metrics_df']
Â Â Â Â styled = df_metrics.style.format({
Â Â Â Â Â Â Â Â col: '{:.4f}' for col in df_metrics.columns if col != 'Model'
Â Â Â Â }).highlight_max(
Â Â Â Â Â Â Â Â subset=['Accuracy', 'F1-Score (macro)', 'Precision (macro)', 'Recall (macro)'],
Â Â Â Â Â Â Â Â color='#d4edda'
Â Â Â Â )
Â Â Â Â st.dataframe(styled, use_container_width=True)
Â Â Â Â melt = df_metrics.melt(id_vars='Model',
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â value_vars=['Accuracy', 'F1-Score (macro)', 'Precision (macro)', 'Recall (macro)'],
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â var_name='Metric', value_name='Score')
Â Â Â Â fig_bar = px.bar(melt, x='Model', y='Score', color='Metric', barmode='group',
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â title="Model Comparison â€“ Key Metrics")
Â Â Â Â fig_bar.update_layout(yaxis_range=[0, 1.05])
Â Â Â Â st.plotly_chart(fig_bar, use_container_width=True)
Â Â Â Â st.markdown(f"### Classification Report â€“ Best Model ({model_results['best_name']})")
Â Â Â Â y_true = model_results['y_test']
Â Â Â Â y_pred_best = model_results['predictions'][model_results['best_name']]
Â Â Â Â report = classification_report(y_true, y_pred_best,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â target_names=model_results['encoder'].classes_,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â output_dict=True, zero_division=0)
Â Â Â Â report_df = pd.DataFrame(report).transpose()
Â Â Â Â st.dataframe(report_df.style.format('{:.4f}'), use_container_width=True)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Tab 4: Sector Prediction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_predict:
Â Â Â Â st.subheader("Predict Dominant Sector")
Â Â Â Â st.caption("Uses the best model from the Performance tab above")
Â Â Â Â c1, c2, c3 = st.columns(3)
Â Â Â Â gdp_val = c1.number_input("GDP (million RM)", 0.0, 500000.0, 50000.0)
Â Â Â Â emp_val = c2.number_input("Total Employment", 1, 1000000, 100000)
Â Â Â Â hours_val = c3.number_input("Total Working Hours", 1.0, 50000000.0, 2000000.0)
Â Â Â Â if st.button("Predict"):
Â Â Â Â Â Â Â Â feat_dict = {
Â Â Â Â Â Â Â Â Â Â Â Â 'gdp': gdp_val,
Â Â Â Â Â Â Â Â Â Â Â Â 'employment': emp_val,
Â Â Â Â Â Â Â Â Â Â Â Â 'hours': hours_val,
Â Â Â Â Â Â Â Â Â Â Â Â 'output_hour': gdp_val / hours_val if hours_val > 0 else 0,
Â Â Â Â Â Â Â Â Â Â Â Â 'output_employment': gdp_val / emp_val if emp_val > 0 else 0,
Â Â Â Â Â Â Â Â Â Â Â Â 'employed_employer': emp_val * 0.10,
Â Â Â Â Â Â Â Â Â Â Â Â 'employed_employee': emp_val * 0.70,
Â Â Â Â Â Â Â Â Â Â Â Â 'employed_own_account': emp_val * 0.15,
Â Â Â Â Â Â Â Â Â Â Â Â 'employed_unpaid_family': emp_val * 0.05
Â Â Â Â Â Â Â Â }
Â Â Â Â Â Â Â Â X_new = pd.DataFrame([feat_dict])
Â Â Â Â Â Â Â Â avail_features = X_new.columns.intersection(model_results['scaler'].feature_names_in_)
Â Â Â Â Â Â Â Â X_new_s = model_results['scaler'].transform(X_new[avail_features])
Â Â Â Â Â Â Â Â pred_enc = model_results['best_model'].predict(X_new_s)[0]
Â Â Â Â Â Â Â Â pred_sector = model_results['encoder'].inverse_transform([pred_enc])[0]
Â Â Â Â Â Â Â Â try:
Â Â Â Â Â Â Â Â Â Â Â Â prob = model_results['best_model'].predict_proba(X_new_s)[0].max() * 100
Â Â Â Â Â Â Â Â Â Â Â Â st.success(f"**Predicted sector: {pred_sector}** ({prob:.1f}% confidence)")
Â Â Â Â Â Â Â Â except:
Â Â Â Â Â Â Â Â Â Â Â Â st.success(f"**Predicted sector: {pred_sector}**")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Tab 5: Data Table
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_data:
Â Â Â Â st.subheader("Filtered Data Table")
Â Â Â Â st.caption(f"Showing data for {year_range[0]}â€“{year_range[1]} and selected sectors")
Â Â Â Â st.dataframe(filtered_data.sort_values(['date', 'sector']), use_container_width=True)
Â Â Â Â st.download_button(
Â Â Â Â Â Â Â Â "Download filtered data (CSV)",
Â Â Â Â Â Â Â Â filtered_data.to_csv(index=False).encode('utf-8'),
Â Â Â Â Â Â Â Â f"malaysia_labour_{year_range[0]}-{year_range[1]}.csv",
Â Â Â Â Â Â Â Â "text/csv"
Â Â Â Â )
st.caption("Dashboard â€¢ Updated January 14, 2026")
